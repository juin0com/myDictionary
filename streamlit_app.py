import os
import streamlit as st
from dotenv import load_dotenv
from langchain import hub
from langchain.agents import AgentExecutor, create_openai_tools_agent, load_tools
from langchain.memory import ConversationBufferMemory
from langchain.schema import HumanMessage
from langchain_community.chat_message_histories import StreamlitChatMessageHistory
from langchain_community.callbacks import StreamlitCallbackHandler
from langchain_openai import ChatOpenAI
from collections.abc import MutableSet


load_dotenv()

def create_agent_chain(history):
    chat = ChatOpenAI(
        model=os.getenv("OPENAI_API_MODEL"),
        temperature=os.getenv("OPENAI_API_TEMPERATURE"),
    )
    tools = load_tools(["ddg-search", "wikipedia"])
    prompt = hub.pull("hwchase17/openai-tools-agent")
    memory = ConversationBufferMemory(
        chat_memory=history, memory_key="chat_key", return_messages=True
    )
    
    agent = create_openai_tools_agent(chat, tools, prompt)
    return AgentExecutor(agent=agent, tools=tools, memory=memory)

st.title("ğŸˆë‹¨ë¹„ë…¸íŠ¸ ì±—ë´‡ì„œë¹„ìŠ¤ğŸˆ")

history = StreamlitChatMessageHistory()
prompt = st.chat_input("""
ë‹¹ì‹ ì€ ì˜ì–´ ì‚¬ì „ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì‚¬ìš©ìê°€ ì…ë ¥í•œ ì˜ì–´ ë‹¨ì–´ì˜ ì •ì˜, ì˜ˆë¬¸, ë°œìŒ ë“±ì„ ì •í™•í•˜ê²Œ ì œê³µí•´ ì£¼ì„¸ìš”.

ì‚¬ìš©ì ì§ˆë¬¸: {input}
""")

if prompt:
    with st.chat_message("user"):
        history.add_user_message(prompt)
        st.markdown(prompt)

    with st.chat_message("assistant"):
        callback = StreamlitCallbackHandler(st.container())
        agent_chain = create_agent_chain(history)
        response = agent_chain.invoke(
            {"input": prompt},
            {"callback": [callback]},
        )
        st.markdown(response["output"])